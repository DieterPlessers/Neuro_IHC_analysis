{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8160eb4",
   "metadata": {},
   "source": [
    "# Analysis of Neuronal Cell Detector (NCD) model\n",
    "\n",
    "## 0. Outline\n",
    "This code deals with the automatic processing of raw data from mouse brains analysed with “Neuronal Cell Detector” model developed in Aiforia® Create. We typically start from Excel/CSV files collected in a local folder on the computer that is specified in the code. To automatically change the format of a series of files, refer to the Change_Name_Format_Input_Data.ipynb notebook. The code is developed to take into account that a mouse brain can be mounted over several slides. Slides for each animal are named identically, except for a numeric postfix denoting the slide number: '_S1', '_S2', etc.\n",
    "\n",
    "The present notebook is divided into 3 sections:\n",
    "\n",
    "**1) Make the necessary functions for part 2 and part 3**\n",
    "\n",
    "\n",
    "**2) Automatic analysis of X * N Slides of N Brains**\n",
    "\n",
    "Here we automate the analysis of all X*N slide images of all N brains (X slides per brain, which is a parameter that the user can choose) in the folder with raw data. The approach is as follows:\n",
    "\n",
    "1) We collect all the X*N names of the raw data files in the folder and put them in a list.\n",
    "\n",
    "2) We make a list containing only the N filenames with an '_S1' in the name. These are the N first slides of the N brains.\n",
    "\n",
    "3) We loop over these N first slide images belonging to the N brains and perform the following steps in each loop:\n",
    "\n",
    "    a) We retrieve the second slide (containing an '_S2' in the filename of the raw data) belonging to this specific brain. We do the same for the third ('_S3'), fourth ('_S4'), ..., X'th ('_SX')  slides of the specific brain.   \n",
    "    b) We perform the data analysis steps on the S1, S2, ..., SX slides separately, and also on the concatenated data of S1+S2+...+SX.     \n",
    "    c) We output the results to an excel file for this specific brain.\n",
    "    \n",
    "After each loop, we add the output of this specific brain to an overview table that will contain all results for all brains. After the last loop, this overview table is also exported to an excel file.\n",
    "\n",
    "**3) Automatic analysis of X * N Slides of N Brains after determining to which hemisphere they belong**\n",
    "\n",
    "Here we add which brain regions are on each hemisphere, and compare the injected vs uninjected sides. Analysis occurs similar to section 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f008181a",
   "metadata": {},
   "source": [
    "## Part 1 - Make the necessary functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b54e0a7",
   "metadata": {},
   "source": [
    "### Part 1.1 - Load all necessary Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f492eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required Python packages\n",
    "import pandas as pd                                # For data analysis with dataframes\n",
    "import math                                        # To get the value for pi\n",
    "import functools                                   # For higher-order functions that work on other functions\n",
    "from IPython.display import display                # Enables the display of more than one dataframe per code cell\n",
    "import numpy as np                                 # For data analysis\n",
    "import glob                                        # To get all raw data file locations\n",
    "import os                                          # To get all raw data file locations\n",
    "pd.options.display.float_format = '{:.2f}'.format  # Display all numbers in dataframes with 2 decimals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f838015",
   "metadata": {},
   "source": [
    "### Part 1.2 - Data locations\n",
    "\n",
    "**TO DO:** \n",
    "- Specify the format of the raw data and the raw data folder location, as well as some experimental parameters.\n",
    "- Specify the file paths of the excel file containing your quality control revisions and the excel file mapping each brain region to a hemisphere.\n",
    "- Specify the folder locations where you would like to collect the output excel files (for whole brain and hemisphere analysis).  \n",
    "\n",
    "The format is: <font color='darkred'>r'file_location'</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db3aec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify what data format you want to use for your raw data: excel, csv or feather. Do this by uncommenting the data_format that you want.\n",
    "data_format = 'csv'\n",
    "# data_format = 'excel'\n",
    "# data_format = 'feather'\n",
    "\n",
    "# Specify the maximal amount of slides you have per animal brain. If this for instance is 4, we expect filenames containing '_S1', '_S2', '_S3' and '_S4'.\n",
    "# If some animal brains have less slides, no problem. The code will create empty data files for the missing slides so it can run properly.\n",
    "amount_of_slides = 4\n",
    "\n",
    "# Specify the experimental parameters (section_thickness in micrometers) and locations:\n",
    "# The spacing parameter refers to the serial section spacing interval. It's the interval at which you sample the brain volume for analysis, not the physical distance between each section. For example, if you have a spacing parameter of 10, you would take every 10th section for your analysis.  \n",
    "spacing=12\n",
    "section_thickness = 40\n",
    "folder_raw_data = r'C:\\Users\\...\\Raw_Data_NCD'\n",
    "file_brainregions_to_replace =  r'C:\\Users\\...\\Brainregions_To_Replace_NCD.xlsx'\n",
    "file_brainregions_injected =  r'C:\\Users\\...\\Brainregions_Hemisphere_NCD.xlsx'\n",
    "folder_output_results = r'C:\\Users\\...\\Results_Wholebrain_NCD'\n",
    "folder_output_results_injected = r'C:\\Users\\...\\Results_Hemisphere_NCD'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cf887c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the output folders if they did not exist yet\n",
    "if not os.path.isdir(folder_output_results):\n",
    "    os.mkdir(folder_output_results)\n",
    "if not os.path.isdir(folder_output_results_injected):\n",
    "    os.mkdir(folder_output_results_injected)\n",
    "\n",
    "# Make the list of filename appendices that are expected. For instance if amount_of_slides = 4, then appendices_list = ['_S1', '_S2', '_S3', '_S4']\n",
    "appendices_list = [f\"_S{i}\" for i in range(1, amount_of_slides + 1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9522c6",
   "metadata": {},
   "source": [
    "### Part 1.3 - Function to load all image files that need to be analyzed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e65479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_file_locations_S1(folder_raw_data):\n",
    "    \"\"\"\n",
    "    Make a list of all file locations for S1 images present in the folder with all raw data files.\n",
    "    It's thus important that the filenames contain '_S1' in their name, even if there is no '_S2' counterparty. \n",
    "    If you don't work with '_S1' and '_S2', then just append '_S1' to the filenames to make the code work.\n",
    "    Output: list of all file locations for S1 images.\n",
    "    \"\"\"\n",
    "    \n",
    "    if data_format == 'csv':\n",
    "        all_raw_data_file_locations = glob.glob(os.path.join(folder_raw_data, \"*.csv\"))\n",
    "    elif data_format == 'excel':\n",
    "        all_raw_data_file_locations = glob.glob(os.path.join(folder_raw_data, \"*.xlsx\"))\n",
    "    elif data_format == 'feather':\n",
    "        all_raw_data_file_locations = glob.glob(os.path.join(folder_raw_data, \"*.feather\"))\n",
    "    else:\n",
    "        print('You did not specify a correct data-format in Part 1.2 and can expect some errors in the rest of the code')\n",
    "        \n",
    "    all_raw_data_file_locations.sort()\n",
    "\n",
    "    print('The location of all the raw data files = ')\n",
    "    for file_location in all_raw_data_file_locations:\n",
    "        print(file_location)\n",
    "\n",
    "    # Extract the file names that contain '_S1' in the file name. These are the N first images of the N unique brains.\n",
    "    all_raw_data_file_locations_S1= [x for x in all_raw_data_file_locations if '_S1' in x]\n",
    "    print('\\nThe location of all the raw S1 data files = ')\n",
    "    for file_location_S1 in all_raw_data_file_locations_S1:\n",
    "        print(file_location_S1)\n",
    "        \n",
    "    return all_raw_data_file_locations_S1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638c5693",
   "metadata": {},
   "source": [
    "### Part 1.4 - Function to load the file with corrections for the brainregions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c618cc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_brainregions_to_replace(file_brainregions_to_replace):\n",
    "    \"\"\"\n",
    "    Load the file containing the corrections for brain regions that need to be replaced for each specific image.\n",
    "    Output: cleaned dataframe with brain regions that need to be replaced for each image.\n",
    "    \"\"\"\n",
    "    \n",
    "    df_brainregions_to_replace_raw=pd.read_excel(file_brainregions_to_replace,\n",
    "                                                 usecols=['Image', 'Brainregion_Wrong', 'Brainregion_Correct'],\n",
    "                                                 dtype={'Image': 'str', 'Brainregion_Wrong': 'str', 'Brainregion_Correct': 'str'}\n",
    "                                                )\n",
    "\n",
    "    # Modify the dataframe to delete spaces that are by accident there, and put the brainregions in upper case \n",
    "    df_brainregions_to_replace=df_brainregions_to_replace_raw.copy()\n",
    "    df_brainregions_to_replace['Image'] = df_brainregions_to_replace_raw['Image'].str.strip()\n",
    "    df_brainregions_to_replace['Brainregion_Wrong'] = df_brainregions_to_replace_raw['Brainregion_Wrong'].str.upper().str.strip()\n",
    "    df_brainregions_to_replace['Brainregion_Correct'] = df_brainregions_to_replace_raw['Brainregion_Correct'].str.upper().str.strip()\n",
    "\n",
    "    #     print('The raw table of the brain regions to replace for each image = ')\n",
    "    #     display(df_brainregions_to_replace_raw)\n",
    "\n",
    "    print('The modified table of the brain regions to replace for each image = ')\n",
    "    display(df_brainregions_to_replace)\n",
    "    \n",
    "    return df_brainregions_to_replace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c26c5f4",
   "metadata": {},
   "source": [
    "### Part 1.5 - Function to load the file with which brainregions were injected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf098c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_brainregions_injected(file_brainregions_injected):\n",
    "    \"\"\"\n",
    "    Load the file specifying which brainregions were on the injected side for each specific image.\n",
    "    Output: cleaned dataframe with brain regions that were injected for each image.\n",
    "    \"\"\"\n",
    "    \n",
    "    df_brainregions_injected_raw=pd.read_excel(file_brainregions_injected,\n",
    "                                               usecols=['Image', 'Brainregion', 'Hemisphere'],\n",
    "                                               dtype={'Image': 'str', 'Brainregion': 'str', 'Hemisphere': 'str'}\n",
    "                                               )\n",
    "    \n",
    "    # Modify the dataframe to delete spaces that are by accident there, and put the brainregions in upper case \n",
    "    df_brainregions_injected=df_brainregions_injected_raw.copy()\n",
    "    df_brainregions_injected['Image'] = df_brainregions_injected_raw['Image'].str.strip()\n",
    "    df_brainregions_injected['Brainregion'] = df_brainregions_injected_raw['Brainregion'].str.upper().str.strip()\n",
    "    df_brainregions_injected['Daughter1_Injected'] = df_brainregions_injected_raw['Hemisphere'].str.upper().str.strip()\n",
    "    df_brainregions_injected.drop(columns=['Hemisphere'], inplace=True)\n",
    "\n",
    "    #     print('The raw table of the brain regions injected for each image = ')\n",
    "    #     display(df_brainregions_injected_raw)\n",
    "\n",
    "    print('The modified table of the brain regions injected for each image = ')\n",
    "    display(df_brainregions_injected)\n",
    "    \n",
    "    return df_brainregions_injected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1d7805",
   "metadata": {},
   "source": [
    "### Part 1.6 - Function to load dataframe and clean it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b0bea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_cleaning(file_location, df_brainregions_to_replace):\n",
    "    \"\"\"\n",
    "    Load the specific file location in a dataframe and clean it with df_brainregions_to_replace.\n",
    "    Output: loaded and cleaned dataframe with some additional calculated values.\n",
    "    \"\"\"\n",
    "    if data_format == 'csv':\n",
    "        # Check if the  file exists. If not, we make an empty CSV file with the right columns:\n",
    "        try:\n",
    "            df_1=pd.read_csv(file_location, sep='\\t',\n",
    "                             usecols=['Image', 'Parent area name', 'Area/object name', 'Class label', 'Area (μm²)', 'Circumference (µm)'],\n",
    "                             dtype={'Image': 'str', 'Parent area name': 'str', 'Area/object name': 'str', \n",
    "                                    'Class label': 'str', 'Area (μm²)': 'float64','Circumference (µm)': 'float64' },\n",
    "                             keep_default_na = True) \n",
    "            \n",
    "        except:\n",
    "            df_empty = pd.DataFrame(columns=['Image', 'Parent area name', 'Area/object name', 'Class label', 'Area (μm²)', 'Circumference (µm)'])\n",
    "            df_empty.reset_index(inplace=True)\n",
    "            df_empty.to_csv(file_location, sep='\\t')\n",
    "            df_1=pd.read_csv(file_location, sep='\\t',\n",
    "                             usecols=['Image', 'Parent area name', 'Area/object name', 'Class label', 'Area (μm²)', 'Circumference (µm)'],\n",
    "                             dtype={'Image': 'str', 'Parent area name': 'str', 'Area/object name': 'str', \n",
    "                                    'Class label': 'str', 'Area (μm²)': 'float64','Circumference (µm)': 'float64' },\n",
    "                             keep_default_na = True) \n",
    "            print(f'\\n A dataframe at location {file_location} did not exist, so we made an empty dataframe.')\n",
    "\n",
    "            \n",
    "    elif data_format == 'excel':\n",
    "        # Check if the  file exists. If not, we make an empty excel file with the right columns:\n",
    "        try:\n",
    "            df_1=pd.read_excel(file_location,\n",
    "                               usecols=['Image', 'Parent area name', 'Area/object name', 'Class label', 'Area (μm²)','Circumference (µm)'],\n",
    "                               dtype={'Image': 'str', 'Parent area name': 'str', 'Area/object name': 'str', \n",
    "                                      'Class label': 'str', 'Area (μm²)': 'float64','Circumference (µm)': 'float64' },\n",
    "                               keep_default_na = True)\n",
    "        except:\n",
    "            df_empty = pd.DataFrame(columns=['Image', 'Parent area name', 'Area/object name', 'Class label', 'Area (μm²)', 'Circumference (µm)'])\n",
    "            df_empty.reset_index(inplace=True)\n",
    "            df_empty.to_excel(file_location)\n",
    "            df_1=pd.read_excel(file_location,\n",
    "                               usecols=['Image', 'Parent area name', 'Area/object name', 'Class label', 'Area (μm²)','Circumference (µm)'],\n",
    "                               dtype={'Image': 'str', 'Parent area name': 'str', 'Area/object name': 'str', \n",
    "                                      'Class label': 'str', 'Area (μm²)': 'float64','Circumference (µm)': 'float64' },\n",
    "                               keep_default_na = True)\n",
    "            print(f'\\n A dataframe at location {file_location} did not exist, so we made an empty dataframe.')\n",
    "            \n",
    "    elif data_format == 'feather':\n",
    "        # Check if the  file exists. If not, we make an empty feather file with the right columns:\n",
    "        try:\n",
    "            df_1=pd.read_feather(file_location) \n",
    "            dtype_dictionary = {'Image': 'object', 'Parent area name': 'object', 'Area/object name': 'object', \n",
    "                                'Class label': 'object', 'Area (μm²)': 'float64','Circumference (µm)': 'float64' }\n",
    "            df_1=df_1.astype(dtype_dictionary)\n",
    "        except:\n",
    "            df_empty = pd.DataFrame(columns=['Image', 'Parent area name', 'Area/object name', 'Class label', 'Area (μm²)', 'Circumference (µm)'])\n",
    "            df_empty.reset_index(inplace=True)\n",
    "            df_empty.to_feather(file_location)\n",
    "            df_1=pd.read_feather(file_location) \n",
    "            dtype_dictionary = {'Image': 'object', 'Parent area name': 'object', 'Area/object name': 'object', \n",
    "                                'Class label': 'object', 'Area (μm²)': 'float64','Circumference (µm)': 'float64' }\n",
    "            df_1=df_1.astype(dtype_dictionary)\n",
    "            print(f'\\n A dataframe at location {file_location} did not exist, so we made an empty dataframe.')\n",
    "\n",
    "    else:\n",
    "        print('You did not specify a correct data-format in Part 1.2 and can expect some errors in the rest of the code')\n",
    "        \n",
    "\n",
    "\n",
    "    # Put all columns in capitals to never make mistakes against capitalization\n",
    "    df_1['Parent area name'] = df_1['Parent area name'].str.upper()\n",
    "    df_1['Area/object name'] = df_1['Area/object name'].str.upper()\n",
    "    df_1['Class label']      = df_1['Class label'].str.upper()\n",
    "\n",
    "    # Get the image name out of the file_path (getting image name from dataframe first column is hard because some are empty, \n",
    "    # and getting from filename makes more sense anyway) and change some field based on the recipe. \n",
    "    full_name = os.path.basename(file_location)\n",
    "    file_name = os.path.splitext(full_name)\n",
    "    image_name = file_name[0]\n",
    "    print('The present image=', image_name)\n",
    "    \n",
    "    # Make sure the image name across the whole first column is correct\n",
    "    df_1['Image']=image_name\n",
    "    \n",
    "    print('The full raw data=')\n",
    "    display(df_1)\n",
    "\n",
    "    # Determine the dictionary of brain regions that should be replaced for this specific image\n",
    "    df_brainregions_to_replace = df_brainregions_to_replace[df_brainregions_to_replace['Image']==image_name]\n",
    "    dict_brainregions_to_replace= pd.Series(df_brainregions_to_replace.Brainregion_Correct.values, index=df_brainregions_to_replace.Brainregion_Wrong).to_dict()\n",
    "\n",
    "    print('The dictionary of brain regions to replace for this specific image', image_name, 'is', dict_brainregions_to_replace)\n",
    "\n",
    "    # Replace the value in the rows that have a Parent area name or Area/object name that is in list_brainregions_replace\n",
    "    df_2=df_1.copy()\n",
    "    df_2['Parent area name'] = df_1['Parent area name'].replace(dict_brainregions_to_replace, regex=False)\n",
    "    df_2['Area/object name'] = df_1['Area/object name'].replace(dict_brainregions_to_replace, regex=False)\n",
    "    \n",
    "    # Create a column 'Parent area name merged' and 'Area/object name merged' where the numbers are deleted from these columns:\n",
    "    df_2['Parent area name merged'] = df_2['Parent area name'].str.replace('\\\\d+', '', regex=True).str.strip()\n",
    "    df_2['Area/object name merged'] = df_2['Area/object name'].str.replace('\\\\d+', '', regex=True).str.strip()\n",
    "\n",
    "    # The rows in which we put a parent empty, can have an area/object name that itself occurs as parent and that should also be deleted\n",
    "    # (basically the Daughter 3 of the parent should also be deleted)\n",
    "    df_empty_parent = df_2[df_2['Parent area name']=='EMPTY']\n",
    "    list_of_area_objects_that_should_be_empty = df_empty_parent['Area/object name'].to_list()\n",
    "    print('list_of_area_objects_that_should_be_empty = ', list_of_area_objects_that_should_be_empty)\n",
    "    df_2.loc[df_2[\"Parent area name\"].isin(list_of_area_objects_that_should_be_empty), \"Parent area name\"] = \"EMPTY\"\n",
    "\n",
    "    # Delete the rows in which we just made the Parent area name or Area/object name 'EMPTY' by replacing them with the dictionary\n",
    "    df_3 = df_2[(df_2['Parent area name']!='EMPTY') &  (df_2['Area/object name']!='EMPTY')]\n",
    "\n",
    "    # Show the full updated dataframe:\n",
    "    print('The fully cleaned table with \"Parent area name merged\", \"Area/object name merged\" and with the replaced brainregions:')\n",
    "    display(df_3)\n",
    "    \n",
    "    return df_3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb5f03c",
   "metadata": {},
   "source": [
    "### Part 1.7 - Function to make hierarchical dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ceabdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The hierarchy: We have 1 type of parent (TISSUE 1, 2, 3 etc) with many types of daughter 1 (Amygdala 1, 2, 3 etc,  Striatum 1, 2, 3 etc) \n",
    "# and 1 type of daughter 2 (NEUN POSITIVE AREA 1010, NEUN POSITIVE AREA 1011 etc)\n",
    "# and 1 type of daughter 3 (NEUN POSITIVE CELL 5495, NEUN POSITIVE CELL 5711 etc)\n",
    "def make_hierarchy(df):\n",
    "    \"\"\" \n",
    "    Here we make the hierarchical structure of the data in the dataframe more clear. \n",
    "    The field 'Parent area name' is always the parent of the 'Area/object name' in the same row. \n",
    "    The area in the row always belongs to the 'Area/object name'.\n",
    "    Output: four dataframes in which gradually more hierarchy is added.\n",
    "    \"\"\"\n",
    "\n",
    "    # The rows with the top parent (= TISSUE X) are the rows that don't have an own Parent area name\n",
    "    df_parent_almost = df[df['Parent area name'].isna()]\n",
    "    dict_parent={'Area/object name':'Parent name', 'Area/object name merged': 'Parent name merged', 'Area (μm²)': 'Area Parent (μm²)'}\n",
    "    df_parent=df_parent_almost.rename(columns=dict_parent)\n",
    "    df_parent.drop(columns=['Parent area name', 'Class label', 'Parent area name merged'], inplace=True)\n",
    "\n",
    "    # Then we add the first daughter = the daughter of the top parents\n",
    "    df_parent_daughter1_almost=df_parent.merge(df[['Parent area name', 'Area/object name', 'Area/object name merged', 'Area (μm²)']], left_on='Parent name', right_on='Parent area name', how='inner')\n",
    "    dict_daughter1 = {'Parent area name': 'Parent name copy', 'Area/object name':'Daughter1', \n",
    "                      'Area/object name merged': 'Daughter1 merged', 'Area (μm²)': 'Area Daughter1 (μm²)'}\n",
    "\n",
    "    df_parent_daughter1_almost2=df_parent_daughter1_almost.rename(columns=dict_daughter1)\n",
    "    # Groupby is needed because there now can be for instance 2 Striatum 4's \n",
    "    # (one of them originated from e.g. changing Amygdala 1 to Striatum 4 in the brainregion corrections)\n",
    "    # We need to turn this Striatum 4 into a unique row because otherwise we will double in the next join when making df_parent_daughter2\n",
    "    df_parent_daughter1=df_parent_daughter1_almost2.groupby(['Daughter1'], as_index=False).agg(\n",
    "        {'Image': 'first', 'Parent name': 'first', 'Area Parent (μm²)': 'first',\n",
    "         'Parent name merged': 'first', 'Parent name copy': 'first', 'Daughter1': 'first',\n",
    "         'Daughter1 merged': 'first', 'Area Daughter1 (μm²)': 'sum'})\n",
    "\n",
    "    # Then we add the second daughter = the daughter of daughter 1\n",
    "    df_parent_daughter2_almost=df_parent_daughter1.merge(df[['Parent area name', 'Area/object name', 'Area/object name merged', 'Area (μm²)']], left_on='Daughter1', right_on='Parent area name', how='inner')\n",
    "    dict_daughter2 = {'Parent area name': 'Daughter1 copy','Area/object name':'Daughter2', \n",
    "                      'Area/object name merged': 'Daughter2 merged', 'Area (μm²)': 'Area Daughter2 (μm²)'}\n",
    "    df_parent_daughter2=df_parent_daughter2_almost.rename(columns=dict_daughter2)\n",
    "\n",
    "    # Then we add the third daughter = the daughter of daughter 2\n",
    "    df_parent_daughter3_almost=df_parent_daughter2.merge(df[['Parent area name', 'Area/object name','Area/object name merged', 'Area (μm²)' ]], left_on='Daughter2', right_on='Parent area name', how='inner')\n",
    "    dict_daughter3 = {'Parent area name': 'Daughter2 copy','Area/object name':'Daughter3', \n",
    "                      'Area/object name merged': 'Daughter3 merged', 'Area (μm²)': 'Area Daughter3 (μm²)'}\n",
    "    df_parent_daughter3=df_parent_daughter3_almost.rename(columns=dict_daughter3)\n",
    "\n",
    "#     print('Original df')\n",
    "#     display(df)\n",
    "#     print('df_parent')\n",
    "#     display(df_parent)\n",
    "#     print('df_parent_daughter1')\n",
    "#     display(df_parent_daughter1)\n",
    "#     print('df_parent_daughter2')\n",
    "#     display(df_parent_daughter2)\n",
    "#     print('df_parent_daughter3')\n",
    "#     display(df_parent_daughter3)\n",
    "    \n",
    "    return df_parent, df_parent_daughter1, df_parent_daughter2, df_parent_daughter3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae65daa",
   "metadata": {},
   "source": [
    "### Part 1.8 - Function to calculate all information for all daughter 1's\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb67fd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_calculations(df1, df2, df3, groupby_column1='Daughter1 merged'):\n",
    "    \"\"\"\n",
    "    Make the main calculations (areas, counts...) based on three dataframes with different hierarchies (df1, df2 and df3), \n",
    "    and based on a groupby columns that can be chosen.\n",
    "    Output: dataframe with all calculations.\n",
    "    \"\"\"\n",
    "\n",
    "    # Count the totalregion area of each Daughter 1 merged (so the second layer in the hierarchy, e.g. total area of Amygdala = Area Amygdala 1 + Area Amygdala 2 + ... Area Amygdala 7)\n",
    "\n",
    "    # First method to do this: just group by over all Area/object name merged and discard the elements that are not \n",
    "    # in the second layer of the hierarchy, namely TISSUE (layer 1), NEUN POSITIVE AREA (layer 3) and NEUN POSITIVE CELL (layer 4)\n",
    "    # df_region_areas_merged_almost = df1.groupby(groupby_column2).sum()['Area (μm²)'].rename_axis('Merged area name').reset_index(name='Total Region Area (μm²)')\n",
    "    # regions_exclude=['TISSUE','NEUN POSITIVE CELL','NEUN POSITIVE AREA']\n",
    "    # df_region_areas_merged1=df_region_areas_merged_almost[~df_region_areas_merged_almost['Merged area name'].isin(regions_exclude)]\n",
    "\n",
    "    # print('The total area of each Daughter1 merged')\n",
    "    # display(df_region_areas_merged1)\n",
    "\n",
    "    # Second method to do this: just group by over all 'Daughter1 merged' in df_parent_daughter1 \n",
    "    # (where those daughter 1 rows have not been multiplied due to another join as is the case in df_parent_daughter2)\n",
    "    df_region_areas_merged2 = df1.groupby(groupby_column1).sum()['Area Daughter1 (μm²)'].rename_axis('Merged area name').reset_index(name='Total Region Area (μm²)')\n",
    "\n",
    "    # print('METHOD 2: The total area of each Daughter1 merged')\n",
    "    # display(df_region_areas_merged2)\n",
    "    \n",
    "\n",
    "    # Count the NeuN Positive Area (layer 3 = Daugter2 area) of the cells belonging to each Daughter1 merged\n",
    "    df_positive_areas_merged = df2.groupby(groupby_column1).sum()['Area Daughter2 (μm²)'].rename_axis('Merged area name').reset_index(name='Total NeuN Positive Area (μm²)')\n",
    "\n",
    "    # Count the number of rows (each row has a cell name like NEUN POSITIVE CELL 29144 in Daughter 3, corresponding to layer 4) for each Daughter 1 merged\n",
    "    df_positive_counts = df3.value_counts(groupby_column1, sort=True).rename_axis('Merged area name').reset_index(name='Counts')\n",
    "    \n",
    "    # print('The total NeuN Positive Area of each Daughter1 merged')\n",
    "    # display(df_positive_areas_merged)\n",
    "    # print('The total NeuN Positive cell count of each Daughter1 merged')       \n",
    "    # display(df_positive_counts)\n",
    "\n",
    "    # Join all these dataframes together on the 'Merged area name' column\n",
    "\n",
    "    dfs_to_merge = [df_region_areas_merged2, df_positive_areas_merged, df_positive_counts]\n",
    "    df_all_calcs_merged  = functools.reduce(lambda left, right: pd.merge(left,right,on='Merged area name', how='outer'), dfs_to_merge)\n",
    "\n",
    "    # Put all calculated results together\n",
    "    df_all_calcs_merged['Extrapolated Cell Count']            = df_all_calcs_merged['Counts']*spacing\n",
    "    df_all_calcs_merged['Percentage NeuN Positive Area'] = df_all_calcs_merged['Total NeuN Positive Area (μm²)']/df_all_calcs_merged['Total Region Area (μm²)']*100\n",
    "    df_all_calcs_merged['Cells/Region Area (per μm²)']          = df_all_calcs_merged['Counts']/df_all_calcs_merged['Total Region Area (μm²)']\n",
    "    df_all_calcs_merged['Cells/Region Volume (per μm³)']        = df_all_calcs_merged['Cells/Region Area (per μm²)']/section_thickness\n",
    "    df_all_calcs_merged['Cells/Region Area (mm²)']          = df_all_calcs_merged['Cells/Region Area (per μm²)']*1000000\n",
    "    df_all_calcs_merged['Cells/Region Volume (mm³)']        = df_all_calcs_merged['Cells/Region Volume (per μm³)']*1000000000\n",
    "    \n",
    "    df_all_calcs_merged.drop(columns=['Cells/Region Area (per μm²)', 'Cells/Region Volume (per μm³)'], inplace=True)\n",
    "    df_all_calcs_merged.sort_values('Merged area name',inplace=True)\n",
    "    \n",
    "    print(f'The total Calculations of each {groupby_column1}')\n",
    "    display(df_all_calcs_merged)\n",
    "    \n",
    "    return df_all_calcs_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c6f554",
   "metadata": {},
   "source": [
    "## Part 2 - Automatic Wholebrain Analysis of all X*N Slides of all N Brains\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3451b796",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time   \n",
    "# For curiosity we measure the time the code in this cell takes to run\n",
    "\n",
    "# Load the modified file with brain regions to replace/delete for each specific image \n",
    "df_brainregions_to_replace=load_data_brainregions_to_replace(file_brainregions_to_replace)\n",
    "\n",
    "# Extract the file names that contain '_S1' in the file name. These are the N first slide images of the N unique brains.\n",
    "all_raw_data_file_locations_S1= load_all_file_locations_S1(folder_raw_data)\n",
    "\n",
    "# We initiate a counter to keep track in which loop we are below:\n",
    "count = 0\n",
    "\n",
    "# Loop over all the S1 pictures in the raw_data folder, corresponding to all N unique brains\n",
    "for file_location_S1 in all_raw_data_file_locations_S1:\n",
    "    count = count +1 # Counts the loop; first loop: counter = 1\n",
    "    \n",
    "    # Get the image name out of the file_path (getting image name from dataframe first column is hard because some are empty)\n",
    "    full_name = os.path.basename(file_location_S1)\n",
    "    file_name = os.path.splitext(full_name)\n",
    "    image_name_S1 = file_name[0]\n",
    "\n",
    "    dict_df_SX_final = {}    # {'_S1' : df_S1_final, '_S2' : df_S2_final, ..., '_SX' : df_SX_final}\n",
    "    dict_df_SX_parent = {}   # {'_S1' : df_S1_parent, '_S2' : df_S2_parent, ..., '_SX' : df_SX_parent}\n",
    "    dict_df_SX_parent_daughter1 = {}\n",
    "    dict_df_SX_parent_daughter2 = {}\n",
    "    dict_df_SX_parent_daughter3 = {}\n",
    "    dict_df_SX_all_calcs_merged = {}\n",
    "    \n",
    "    for appendix in appendices_list:   \n",
    "        # appendix is in ['_S1', '_S2', '_S3', ... , '_SX']\n",
    "        file_location = file_location_S1.replace('_S1', appendix)\n",
    "    \n",
    "        # Do the data cleaning, making use of the functions defined above\n",
    "        print('\\n Analysis of ', file_location)\n",
    "        dict_df_SX_final[appendix] = dataframe_cleaning(file_location, df_brainregions_to_replace)\n",
    "        dict_df_SX_parent[appendix], dict_df_SX_parent_daughter1[appendix], dict_df_SX_parent_daughter2[appendix], dict_df_SX_parent_daughter3[appendix] = make_hierarchy(dict_df_SX_final[appendix])\n",
    "\n",
    "        # Do all the calculations, making use of the functions defined above. \n",
    "        # The except part is for when we have made an empty dataframe because no dataframe was available (will never be the case for appendix =_S1).\n",
    "        try: \n",
    "            dict_df_SX_all_calcs_merged[appendix] = all_calculations(dict_df_SX_parent_daughter1[appendix], dict_df_SX_parent_daughter2[appendix], dict_df_SX_parent_daughter3[appendix])\n",
    "            print(f\"All calculations together for {appendix} for {file_location}\")\n",
    "            display(dict_df_SX_all_calcs_merged[appendix])\n",
    "        except:\n",
    "            pass\n",
    "   \n",
    "\n",
    "    # Concatenate the full dataframes of all S1, S2, ..., SX\n",
    "    print('\\n Analysis of all SX files of', image_name_S1)\n",
    "    df_SX_final_concat            = pd.concat(dict_df_SX_final.values(), axis=0)\n",
    "    df_SX_parent_daughter1_concat = pd.concat(dict_df_SX_parent_daughter1.values(), axis=0)\n",
    "    df_SX_parent_daughter2_concat = pd.concat(dict_df_SX_parent_daughter2.values(), axis=0)\n",
    "    df_SX_parent_daughter3_concat = pd.concat(dict_df_SX_parent_daughter3.values(), axis=0)\n",
    "\n",
    "    # Do all the S1 + S2 + ... + SX calculations, making use of the functions defined above. \n",
    "    # The except part is for when we have made an empty dataframe because no dataframe was available (will never be the case for S1+S2).\n",
    "    try: \n",
    "        df_SX_all_calcs_concat = all_calculations(df_SX_parent_daughter1_concat, df_SX_parent_daughter2_concat, df_SX_parent_daughter3_concat)\n",
    "        print('All calculations together for S1+S2+...+SX concatenated for ', file_location_S1)\n",
    "        display(df_SX_all_calcs_concat)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    # Output the results to an excel file that is created in the output folder specified at the beginning of this notebook.\n",
    "    output_file_name_SX = image_name_S1.replace('_S1', '_SX') + '_Results.xlsx'\n",
    "    output_file_location_SX = os.path.join(folder_output_results, output_file_name_SX)\n",
    " \n",
    "    with pd.ExcelWriter(output_file_location_SX) as writer:\n",
    "        for appendix in appendices_list:   \n",
    "        # appendix is in ['_S1', '_S2', '_S3', ... , '_SX']\n",
    "            try:\n",
    "                dict_df_SX_all_calcs_merged[appendix].to_excel(writer, sheet_name=appendix[1:]+'_Results', index=False, float_format = \"%.3f\")\n",
    "            except:\n",
    "                pass  # No SX dataframe was available, and the empty one would lead to errors in the try clause\n",
    "        \n",
    "        df_SX_all_calcs_concat.to_excel(writer, sheet_name='SX_Combined_Results', index=False, float_format = \"%.3f\")\n",
    "    \n",
    "    \n",
    "    # For the overview excel file, only the df_SX_all_calcs_concat dataframe is needed. \n",
    "    # We will make 1 overview excelfiles with a few tabpages that we store in dictionary_overview_dataframes:\n",
    "    # dictionary_overview_dataframes = {Total Region Area: df, Percentage NeuN Positive Area: df, Extrapolated Cell Count:df, Cells/Region Area:df, .... }\n",
    "    \n",
    "    # In the first loop we initiate an empty overview dictionary that will be filled with dataframes. \n",
    "    if count==1:\n",
    "        dictionary_overview_dataframes={}\n",
    "\n",
    "    # Prepare the dataframes that are needed for the overview excel file: choose the needed columns,\n",
    "    # and rename the header of the column with the values to the image_name \n",
    "    # (we go from e.g. image_name_S1 = 131297-1_S1_NeuN to column name = 131297-1)\n",
    "    list_calculation_results=['Total Region Area (μm²)', 'Counts', 'Total NeuN Positive Area (μm²)', 'Extrapolated Cell Count', 'Cells/Region Area (mm²)', \n",
    "                              'Cells/Region Volume (mm³)', 'Percentage NeuN Positive Area']\n",
    "    \n",
    "    # print('list_calculation_results = ', list_calculation_results)\n",
    "    \n",
    "    for calculation_result in list_calculation_results:\n",
    "        df_SX_all_calcs_concat_calculation= df_SX_all_calcs_concat[['Merged area name', calculation_result]].copy()\n",
    "        df_SX_all_calcs_concat_calculation.rename(columns={calculation_result: image_name_S1.replace('_NeuN_S1', '')}, inplace=True)\n",
    "    \n",
    "        if count==1:\n",
    "            # In the first loop we fill the empty overview dictionary with a dataframe with the values calculated in loop 1 \n",
    "            dictionary_overview_dataframes[calculation_result]  = df_SX_all_calcs_concat_calculation.copy()\n",
    "\n",
    "        elif count > 1 :\n",
    "            # In the subsequent loops we will add the values of those loops to the dataframes in the overview dictionary\n",
    "            dictionary_overview_dataframes[calculation_result] = dictionary_overview_dataframes[calculation_result].merge(df_SX_all_calcs_concat_calculation, how='outer', on='Merged area name')\n",
    "\n",
    "    # At the end, we delete some of the dataframes, to ensure they cannot be used in the next loop\n",
    "    del(dict_df_SX_final)\n",
    "    del(dict_df_SX_parent)\n",
    "    del(dict_df_SX_parent_daughter1)\n",
    "    del(dict_df_SX_parent_daughter2)\n",
    "    del(dict_df_SX_parent_daughter3)\n",
    "    del(df_SX_all_calcs_concat)\n",
    "\n",
    "    \n",
    "# After the for loops, we print the final overview tables\n",
    "# Output the final overview tables to an excel file Overview_NeuN_Results.xlsx that is created in the output folder specified at the beginning of this notebook\n",
    "output_file_name_overview = os.path.join(folder_output_results, 'Overview_NeuN_Results.xlsx')\n",
    "    \n",
    "list_calculation_results=['Total Region Area (μm²)', 'Total NeuN Positive Area (μm²)',\n",
    "                          'Counts', 'Extrapolated Cell Count', \n",
    "                          'Cells/Region Area (mm²)', 'Cells/Region Volume (mm³)', \n",
    "                          'Percentage NeuN Positive Area'\n",
    "                          ]\n",
    "\n",
    "with pd.ExcelWriter(output_file_name_overview) as writer: \n",
    "    for calculation_result in list_calculation_results:\n",
    "        calculation_result_clean = calculation_result.replace('/', ' per ').replace('Volume', 'Vol')\n",
    "        \n",
    "        print(f'Overview dataframe with all {calculation_result_clean} for all brains')\n",
    "        display(dictionary_overview_dataframes[calculation_result])\n",
    "\n",
    "        dictionary_overview_dataframes[calculation_result].to_excel(writer, sheet_name=calculation_result_clean, index=False, float_format = \"%.3f\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf99a802",
   "metadata": {},
   "source": [
    "## Part 3 - Automatic Hemisphere Analysis of all X*N Slides of all N Brains (injected vs uninjected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e710fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time   \n",
    "# For curiosity we measure the time the code in this cell takes to run\n",
    "\n",
    "# Load the modified file with brain regions to replace/delete for each specific image \n",
    "df_brainregions_to_replace=load_data_brainregions_to_replace(file_brainregions_to_replace)\n",
    "\n",
    "# Load the modified file with hemisphere analysis for each specific image \n",
    "df_brainregions_injected=load_data_brainregions_injected(file_brainregions_injected)\n",
    "\n",
    "# Extract the file names that contain '_S1' in the file name. These are the N first images of the N unique brains.\n",
    "all_raw_data_file_locations_S1= load_all_file_locations_S1(folder_raw_data)\n",
    "\n",
    "# We initiate a counter to keep track in which loop we are below:\n",
    "count = 0\n",
    "\n",
    "# Loop over all the S1 pictures in the raw_data folder, corresponding to the N unique brains\n",
    "for file_location_S1 in all_raw_data_file_locations_S1:\n",
    "    count = count +1 # Counts the loop; first loop: counter = 1\n",
    "    \n",
    "    # Get the image name out of the file_path (getting image name from dataframe first column is hard because some are empty)\n",
    "    full_name = os.path.basename(file_location_S1)\n",
    "    file_name = os.path.splitext(full_name)\n",
    "    image_name_S1 = file_name[0]\n",
    "\n",
    "    dict_df_SX_final = {}    # {'_S1' : df_S1_final, '_S2' : df_S2_final, ..., '_SX' : df_SX_final}\n",
    "    dict_df_SX_parent = {}   # {'_S1' : df_S1_parent, '_S2' : df_S2_parent, ..., '_SX' : df_SX_parent}\n",
    "    dict_df_SX_parent_daughter1 = {}\n",
    "    dict_df_SX_parent_daughter2 = {}\n",
    "    dict_df_SX_parent_daughter3 = {}\n",
    "    dict_df_SX_all_calcs_merged = {}\n",
    "    \n",
    "    for appendix in appendices_list:   \n",
    "        # appendix is in ['_S1', '_S2', '_S3', ... , '_SX']\n",
    "        file_location = file_location_S1.replace('_S1', appendix)\n",
    "    \n",
    "        # Do the data cleaning, making use of the functions defined above\n",
    "        print('\\n Analysis of ', file_location)\n",
    "        dict_df_SX_final[appendix] = dataframe_cleaning(file_location, df_brainregions_to_replace)\n",
    "        dict_df_SX_parent[appendix], dict_df_SX_parent_daughter1[appendix], dict_df_SX_parent_daughter2[appendix], dict_df_SX_parent_daughter3[appendix] = make_hierarchy(dict_df_SX_final[appendix])\n",
    "\n",
    "    # Concatenate the full dataframes of all S1, S2, ..., SX\n",
    "    print('\\n Analysis of all SX files of', image_name_S1)\n",
    "    df_SX_final_concat            = pd.concat(dict_df_SX_final.values(), axis=0)\n",
    "    df_SX_parent_daughter1_concat = pd.concat(dict_df_SX_parent_daughter1.values(), axis=0)\n",
    "    df_SX_parent_daughter2_concat = pd.concat(dict_df_SX_parent_daughter2.values(), axis=0)\n",
    "    df_SX_parent_daughter3_concat = pd.concat(dict_df_SX_parent_daughter3.values(), axis=0)\n",
    "\n",
    "    # Start from the fully concatenated dataframes of S1, S2... SX after replacing the wrong brainregions\n",
    "    # For those dataframes, we will determine for each row wheter Daughter 1 was Injected or Uninjected. \n",
    "    # The brainregions we don't care about will be deleted because we are doing an inner join\n",
    "    df_SX_final_injected            = df_SX_final_concat.merge(df_brainregions_injected, left_on=['Image', 'Area/object name'], right_on=['Image', 'Brainregion'], how='inner')\n",
    "    df_SX_parent_daughter1_injected = df_SX_parent_daughter1_concat.merge(df_brainregions_injected, left_on=['Image', 'Daughter1'], right_on=['Image', 'Brainregion'], how='inner')\n",
    "    df_SX_parent_daughter2_injected = df_SX_parent_daughter2_concat.merge(df_brainregions_injected, left_on=['Image', 'Daughter1'], right_on=['Image', 'Brainregion'], how='inner')\n",
    "    df_SX_parent_daughter3_injected = df_SX_parent_daughter3_concat.merge(df_brainregions_injected, left_on=['Image', 'Daughter1'], right_on=['Image', 'Brainregion'], how='inner')\n",
    "\n",
    "    \n",
    "    # Do all the S1+ S2...+ SX INJECTED calculations, making use of the functions defined above. \n",
    "    # The except part is for when we have made an empty dataframe because no dataframe was available (will never be the case for S1+S2 +...SX).\n",
    "    try: \n",
    "        df_SX_all_calcs_injected = all_calculations(df_SX_parent_daughter1_injected, df_SX_parent_daughter2_injected, df_SX_parent_daughter3_injected, groupby_column1='Daughter1_Injected')\n",
    "        df_SX_all_calcs_injected.sort_values('Merged area name', ascending=False, inplace=True)\n",
    "        print('All calculations together for all SX INJECTED for ', file_location_S1)\n",
    "        display(df_SX_all_calcs_injected)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "  \n",
    "    # Output the results to an excel file that is created in the output folder specified at the beginning of this notebook.\n",
    "    output_file_name_SX = image_name_S1.replace('_S1', '_SX') + '_Hemisphere_Results.xlsx'\n",
    "    output_file_location_SX = os.path.join(folder_output_results_injected, output_file_name_SX)\n",
    " \n",
    "    with pd.ExcelWriter(output_file_location_SX) as writer:\n",
    "        df_SX_all_calcs_injected.to_excel(writer, sheet_name='SX_Hemisphere_Results', index=False, float_format = \"%.3f\")\n",
    "        \n",
    "   \n",
    "    # For the overview excel file, only the df_SX_all_calcs_injected dataframe is needed. \n",
    "    # We will make 1 overview excelfiles with a few tabpages that we store in dictionary_overview_dataframes_injected:\n",
    "    # dictionary_overview_dataframes_injected = {Total Region Area: df, Percentage NeuN Positive Area: df, Extrapolated Cell Count:df, Cells/Region Area:df, .... }\n",
    "\n",
    "    # In the first loop we initiate an empty overview dictionary that will be filled with dataframes. \n",
    "    if count==1:\n",
    "        dictionary_overview_dataframes_injected={}\n",
    "        \n",
    "    # Prepare the dataframes that are needed for the overview excel file: choose the needed columns,\n",
    "    # and rename the header of the column with the values to the image_name \n",
    "    # (we go from e.g. image_name_S1 = 131297-1_S1_NeuN to column name = 131297-1)\n",
    "    list_calculation_results=['Total Region Area (μm²)', 'Total NeuN Positive Area (μm²)', 'Counts', 'Extrapolated Cell Count', 'Cells/Region Area (mm²)', \n",
    "                              'Cells/Region Volume (mm³)', 'Percentage NeuN Positive Area']\n",
    "    \n",
    "    # print('list_calculation_results = ', list_calculation_results)\n",
    "    \n",
    "    for calculation_result in list_calculation_results:\n",
    "        df_SX_all_calcs_injected_calculation= df_SX_all_calcs_injected[['Merged area name', calculation_result]].copy()\n",
    "        df_SX_all_calcs_injected_calculation.rename(columns={calculation_result: image_name_S1.replace('_NeuN_S1', '')}, inplace=True)\n",
    "    \n",
    "        if count==1:\n",
    "            # In the first loop we fill the empty overview dictionary with a dataframe with the values calculated in loop 1 \n",
    "            dictionary_overview_dataframes_injected[calculation_result]  = df_SX_all_calcs_injected_calculation.copy()\n",
    "\n",
    "        elif count > 1 :\n",
    "            # In the subsequent loops we will add the values of those loops to the dataframes in the overview dictionary\n",
    "            dictionary_overview_dataframes_injected[calculation_result] = dictionary_overview_dataframes_injected[calculation_result].merge(df_SX_all_calcs_injected_calculation, how='outer', on='Merged area name')\n",
    "\n",
    "    # At the end, we delete some of the dataframes, to ensure they cannot be used in the next loop\n",
    "    del(dict_df_SX_final)\n",
    "    del(dict_df_SX_parent)\n",
    "    del(dict_df_SX_parent_daughter1)\n",
    "    del(dict_df_SX_parent_daughter2)\n",
    "    del(dict_df_SX_parent_daughter3)\n",
    "    del(df_SX_all_calcs_injected)\n",
    "\n",
    "    \n",
    "# After the for loops, we print the final overview tables\n",
    "# Output the final overview tables to an excel file Overview_NeuN_Hemisphere_Results.xlsx that is created in the output folder specified at the beginning of this notebook\n",
    "output_file_name_overview = os.path.join(folder_output_results_injected, 'Overview_NeuN_Hemisphere_Results.xlsx')\n",
    "    \n",
    "list_calculation_results=['Total Region Area (μm²)', 'Total NeuN Positive Area (μm²)',\n",
    "                          'Counts', 'Extrapolated Cell Count', \n",
    "                          'Cells/Region Area (mm²)', 'Cells/Region Volume (mm³)', \n",
    "                          'Percentage NeuN Positive Area'\n",
    "                          ]    \n",
    "with pd.ExcelWriter(output_file_name_overview) as writer: \n",
    "    for calculation_result in list_calculation_results:\n",
    "        calculation_result_clean = calculation_result.replace('/', ' per ').replace('Volume', 'Vol')\n",
    "        \n",
    "        print(f'Overview dataframe with all {calculation_result_clean} for all brains')\n",
    "        display(dictionary_overview_dataframes_injected[calculation_result])\n",
    "\n",
    "        dictionary_overview_dataframes_injected[calculation_result].to_excel(writer, sheet_name=calculation_result_clean, index=False, float_format = \"%.3f\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
